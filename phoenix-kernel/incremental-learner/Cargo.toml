[package]
name = "incremental-learner"
version = "1.0.0"
edition = "2021"
authors = ["Phoenix Project Team"]
description = "Incremental Learning Daemon for the Phoenix AGI Kernel"
license = "MIT"

[dependencies]
# Phoenix dependencies
phoenix-common = { path = "../phoenix-common" }
world-self-model = { path = "../world-self-model" }
plastic-ltm = { path = "../plastic-ltm" }
triune-conscience = { path = "../triune-conscience" }

# Async runtime
tokio = { version = "1.35", features = ["full"] }
futures = "0.3"
async-trait = "0.1"

# Machine learning
# NOTE: `tch` is patched in the workspace to a pure-Rust stub under `crates/tch`.
# We do not request any libtorch-related features in the resurrection profile.
tch = "0.13"
# Previous revisions also pulled in the Burn stack (`burn`, `burn-autodiff`,
# `burn-tensor`, `burn-tch`) alongside direct `tch` usage, which created
# `torch-sys` conflicts. Those dependencies remain removed.

# Neural architecture
lora-rs = { path = "../../crates/lora-rs" }
# NOTE: Using local stub implementation of lora-rs during resurrection phase.
# The stub provides type-safe no-op implementations of core LoRA functionality.

# Optimization
rayon = "1.8"
num_cpus = "1.16"

# Error handling
thiserror = "1.0"
anyhow = "1.0"

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
bincode = "1.3"

# Metrics and monitoring
prometheus = "0.13"
opentelemetry = { version = "0.20", features = ["rt-tokio"] }
tracing = "0.1"
tracing-subscriber = "0.3"

# Utilities
parking_lot = "0.12"
rand = "0.8"
chrono = { version = "0.4", features = ["serde"] }

[dev-dependencies]
tokio-test = "0.4"
proptest = "1.4"
test-log = "0.2"
pretty_assertions = "1.4"
# NOTE: `criterion` was removed to simplify dependency resolution and avoid
# tight coupling to a specific `once_cell` version during the initial
# kernel resurrection. Benchmarking can be reintroduced later with a
# carefully pinned toolchain once core build stability is ensured.

[features]
default = []

# Enable CUDA support
# NOTE: The upstream `tch` crate currently does not expose a `cuda`
# feature that matches the previous `tch/cuda` mapping. For now this
# `cuda` feature is a documented no-op so the public feature surface
# remains stable; real GPU support can be wired in later.
cuda = []

# Enable distributed learning
distributed = []

# Enable memory replay optimization
optimized-replay = []

[package.metadata.docs.rs]
all-features = true
rustdoc-args = ["--cfg", "docsrs"]

[package.metadata.phoenix]
component-type = "learning"
safety-critical = true
requires-audit = true
memory-intensive = true
