[package]
name = "world-self-model"
version = "1.0.0"
edition = "2021"
authors = ["Phoenix Project Team"]
description = "World and Self Model for the Phoenix AGI Kernel"
license = "MIT"

[dependencies]
# Phoenix dependencies
phoenix-common = { path = "../phoenix-common" }
plastic-ltm = { path = "../plastic-ltm" }

# Async runtime
tokio = { version = "1.35", features = ["full"] }
futures = "0.3"
async-trait = "0.1"

# Machine learning
# NOTE: `tch` is routed to an internal pure-Rust stub via [patch.crates-io]
# in the workspace manifest. We do not enable any libtorch-related features.
tch = "0.13"
# Experimental tokenizer/HTM integrations are disabled in the resurrection
# profile; the current model implementation uses only simple Rust types.

# Neural architecture
# NOTE: Earlier revisions pulled in the Burn stack (`burn`, `burn-autodiff`,
# `burn-tensor`, `burn-tch`) in addition to direct `tch` usage. This caused a
# `torch-sys` conflict because `burn-tch` requires `tch` 0.14 while the rest of
# the kernel standardizes on `tch` 0.13. The current world-self-model
# implementation only uses `tch` APIs directly, so the Burn dependencies are
# removed to keep a single `tch` version in the workspace.

# Optimization
rayon = "1.8"

# Error handling
thiserror = "1.0"
anyhow = "1.0"

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
bincode = "1.3"

# Cryptography
sha3 = "0.10"

# Metrics and monitoring
prometheus = "0.13"
opentelemetry = { version = "0.20", features = ["rt-tokio"] }
tracing = "0.1"
tracing-subscriber = "0.3"

# Utilities
parking_lot = "0.12"
rand = "0.8"
ndarray = "0.15"
petgraph = "0.6"

[dev-dependencies]
tokio-test = "0.4"
proptest = "1.4"
test-log = "0.2"
pretty_assertions = "1.4"

[features]
default = []

# Enable CUDA support
# NOTE: The upstream `tch` crate currently does not expose a `cuda`
# feature matching the previous `tch/cuda` mapping. For now we keep
# the `cuda` feature as a documented no-op so that the public feature
# surface remains stable, and will wire it to real GPU support once a
# compatible backend is selected.
cuda = []

# Enable model quantization
quantization = []

# Enable distributed training
distributed = []

[package.metadata.docs.rs]
all-features = true
rustdoc-args = ["--cfg", "docsrs"]

[package.metadata.phoenix]
component-type = "model"
safety-critical = true
requires-audit = true
memory-intensive = true
