name: Performance Benchmarks

on:
  # Run on every nightly build at 2 AM UTC
  schedule:
    - cron: '0 2 * * *'
  
  # Allow manual triggering for testing purposes
  workflow_dispatch:
    inputs:
      platform:
        description: 'Platform identifier (macbook_2018, macbook_m3max, windows_gaming, ubuntu_server)'
        required: false
        default: ''
      skip_failure:
        description: 'Skip CI failure on regression (true/false)'
        required: false
        default: 'false'

jobs:
  benchmark:
    name: Run Performance Benchmarks
    # Run the benchmarks on all supported platforms
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        include:
          - os: macos-12
            name: "macOS 2018 MacBook Pro"
            rust: stable
            platform: macbook_2018
          - os: macos-14
            name: "macOS M3 Max"
            rust: stable
            platform: macbook_m3max
          - os: windows-latest
            name: "Windows Gaming"
            rust: stable
            platform: windows_gaming
          - os: ubuntu-24.04
            name: "Ubuntu Server"
            rust: stable
            platform: ubuntu_server
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup Rust Toolchain
        uses: actions-rs/toolchain@v1
        with:
          profile: minimal
          toolchain: ${{ matrix.rust }}
          override: true
          components: rustfmt, clippy

      - name: Cache Dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: ${{ runner.os }}-cargo-

      # Install system dependencies based on platform
      - name: Install macOS Dependencies
        if: startsWith(matrix.os, 'macos')
        run: |
          brew update
          brew install pkg-config

      - name: Install Ubuntu Dependencies
        if: startsWith(matrix.os, 'ubuntu')
        run: |
          sudo apt-get update
          sudo apt-get install -y pkg-config libssl-dev

      # Build the release binaries first (for binary size benchmark)
      - name: Build Release Binary
        uses: actions-rs/cargo@v1
        with:
          command: build
          args: --release

      # Run the benchmark suite
      - name: Run Performance Benchmarks
        uses: actions-rs/cargo@v1
        with:
          command: run
          args: --release --bin phoenix-perf-benchmark -- --platform ${{ matrix.platform }} --save-report

      # Create results directory for artifacts
      - name: Create Results Directory
        run: |
          mkdir -p benchmark-results

      # Move benchmark results to a common location
      - name: Collect Benchmark Results
        run: |
          cp -r ~/Desktop/phoenix-benchmarks-*/* benchmark-results/
        shell: bash

      # Upload the benchmark results as artifacts
      - name: Upload Benchmark Results
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results-${{ matrix.platform }}
          path: benchmark-results/
          retention-days: 30

      # Parse the benchmark results to determine if there's a regression
      - name: Check for Regressions
        id: check-regression
        run: |
          if [ -f benchmark-results/regression-detected.txt ]; then
            echo "regression=true" >> $GITHUB_OUTPUT
          else
            echo "regression=false" >> $GITHUB_OUTPUT
          fi
        shell: bash

      # Fail the workflow if regression is detected (only for scheduled runs)
      - name: Fail on Regression
        if: |
          steps.check-regression.outputs.regression == 'true' && 
          github.event_name == 'schedule' && 
          github.event.inputs.skip_failure != 'true'
        run: |
          echo "Performance regression detected! See benchmark results for details."
          exit 1

  # Create a summary report from all platforms
  summarize:
    name: Create Summary Report
    needs: benchmark
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      # Download all benchmark results
      - name: Download Benchmark Results
        uses: actions/download-artifact@v3
        with:
          path: all-benchmark-results

      # Generate a combined summary report
      - name: Generate Summary Report
        run: |
          echo "# Phoenix Orch Performance Benchmark Summary" > summary.md
          echo "Run Date: $(date)" >> summary.md
          echo "" >> summary.md
          echo "| Platform | Overall Grade | Cold Start | Vector KB | Stress Test |" >> summary.md
          echo "|----------|---------------|------------|-----------|-------------|" >> summary.md
          
          # For each platform's results, extract key metrics and add to summary
          for platform_dir in all-benchmark-results/benchmark-results-*; do
            platform=$(basename "$platform_dir" | cut -d'-' -f3-)
            # Extract metrics from the report file (this is simplified, real version would parse properly)
            if [ -f "$platform_dir/phoenix_benchmark_report.txt" ]; then
              grade=$(grep "Overall grade" "$platform_dir/phoenix_benchmark_report.txt" | awk '{print $NF}')
              cold_start=$(grep "Cold start" "$platform_dir/phoenix_benchmark_report.txt" | awk '{print $3" "$4}')
              vector_kb=$(grep "Vector KB search" "$platform_dir/phoenix_benchmark_report.txt" | awk '{print $5" "$6}')
              stress_test=$(grep "Stress test" "$platform_dir/phoenix_benchmark_report.txt" | awk '{print $3" "$4" "$5" "$6" "$7" "$8}')
              echo "| $platform | $grade | $cold_start | $vector_kb | $stress_test |" >> summary.md
            fi
          done
          
          cat summary.md

      # Upload the summary as an artifact
      - name: Upload Summary Report
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-summary
          path: summary.md
          retention-days: 30

      # If running in a PR context, comment the summary
      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('summary.md', 'utf8');
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });